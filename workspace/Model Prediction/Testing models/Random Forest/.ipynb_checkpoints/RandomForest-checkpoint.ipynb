{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veruska/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.externals import joblib\n",
    "import csv\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1432633, 423)\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../data/input/integrated_data_dummy.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.shape)\n",
    "data = data.sort_values([\"busCode\",\"busCodeSB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of data by day\n",
    "data.groupby(\"DAY(gps_datetime)\")[\"DAY(gps_datetime)\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.busBunching == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING HIGHER HEADWAYS (2% of the data)\n",
    "# two_hours = 120\n",
    "# data = data[data.headway <= two_hours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['headway']\n",
    "bb_col = ['busBunching']\n",
    "hd_threshold = [\"headwayThreshold\"]\n",
    "features = list(set(list(data.columns))-set(target_col)-set(bb_col)-set(hd_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['busBunching']\n",
    "\n",
    "cols = ['headway', 'busBunching', \"headwayThreshold\"]\n",
    "data.drop(columns=cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label column and remove it from data\n",
    "y = data['busBunching']\n",
    "#y_threshold = data['headwayThreshold']\n",
    "\n",
    "data.drop('headway', axis=1, inplace=True)\n",
    "data.drop('busBunching', axis=1, inplace=True)\n",
    "data.drop('headwayThreshold', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "min_max_scaler = preprocessing.MinMaxScaler(copy=False)\n",
    "data_scale = min_max_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/output/normalized_data_X.csv'\n",
    "y_path = '../../data/outputt/y.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "y = pd.read_csv(y_path)\n",
    "print(data.shape)\n",
    "print(y.shape)\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making training and test data: 80% Training, 20% Test\n",
    "random.seed(15) #to get always the same set\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_scale, y, test_size=0.20, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model, X_train, y_train):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "\n",
    "# function to plot the RMSE vs parameter value\n",
    "def plot_rmse_param(series, param_name):\n",
    "    series.plot(title = \"Validation Error vs \" + param_name)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    \n",
    "# function to get the best RMSE and the best parameter value of the model\n",
    "def best_rmse_param(series):\n",
    "    best_rmse = series.min()\n",
    "    best_param = series.idxmin() \n",
    "    \n",
    "    return(best_rmse, best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "n_estimators = [10, 50, 100] #number of trees\n",
    "cv_rf_rmse = [rmse_cv(RandomForestClassifier(n_estimators = n, n_jobs=-1), train_X, train_Y).mean() \n",
    "            for n in n_estimators]\n",
    "\n",
    "series = pd.Series(cv_rf_rmse, index = n_estimators)\n",
    "plot_rmse_param(series, \"n_estimators\")\n",
    "best_rmse_rf, best_estimator_rf = best_rmse_param(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_samples_split = [5, 10, 15, 20, 25]\n",
    "cv_rf_rmse = [rmse_cv(RandomForestClassifier(n_estimators = best_estimator_rf, min_samples_split = n, n_jobs=-1), \n",
    "                      train_X, train_Y).mean() \n",
    "            for n in n_min_samples_split]\n",
    "\n",
    "series = pd.Series(cv_rf_rmse, index = n_min_samples_split)\n",
    "plot_rmse_param(series, \"n_min_samples_split\")\n",
    "best_rmse_rf, best_split_rf = best_rmse_param(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [0.25, 0.33, 0.5, 0.75, 0.8]\n",
    "cv_rf_rmse = [rmse_cv(RandomForestClassifier(n_estimators=best_estimator_rf, min_samples_split=best_split_rf,\n",
    "                                            max_features=n, n_jobs=-1), \n",
    "                      train_X, train_Y).mean() \n",
    "            for n in n_features]\n",
    "\n",
    "series = pd.Series(cv_rf_rmse, index = n_features)\n",
    "plot_rmse_param(series, \"max_features\")\n",
    "best_rmse_rf, best_max_feat_rf = best_rmse_param(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_rf = 100\n",
    "best_split_rf = 5\n",
    "best_max_feat_rf = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "try:\n",
    "    start\n",
    "except NameError: # start does not exist at all\n",
    "    start = time.time()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=best_estimator_rf, min_samples_split=best_split_rf,\n",
    "                           max_features=best_max_feat_rf, n_jobs=-1)\n",
    "rf.fit(train_X, train_Y)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution time: \" + str((end - start)/60) + \" min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a pickle file for the model\n",
    "joblib.dump(rf, 'Saved_RF_100_5_75_BB_class.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 coefficient (score) of determination is a statistical measure of how well the regression predictions approximate the real data points. The best one is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = rf.score(test_X, test_Y)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(r2) + \" of the data is been explained by the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_estimator_rf: \" + str(best_estimator_rf))\n",
    "print(\"best_split_rf: \" + str(best_split_rf))\n",
    "print(\"best_max_features_rf: \" + str(best_max_feat_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array = rf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# rf_load = joblib.load('Saved_RF_100_5_075.pkl')\n",
    "# pred_array = rf_load.predict(test_X)\n",
    "\n",
    "# removing the array of each element\n",
    "pred = []\n",
    "for p in pred_array:\n",
    "    pred.append(p)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(test_Y, pred))\n",
    "print(rmse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(pred))\n",
    "print(max(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = y_threshold[test_Y.index]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_pred = np.less_equal(pred, alpha)\n",
    "bb_label = np.less_equal(test_Y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bb = []\n",
    "for x in pred:\n",
    "    if x <= 0.5:\n",
    "        pred_bb.append(0)\n",
    "    else:\n",
    "        pred_bb.append(1)\n",
    "\n",
    "pred_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bus Bunching\n",
    "accuracy = accuracy_score(test_Y, pred_bb)\n",
    "precision = precision_score(test_Y, pred_bb)\n",
    "recall = recall_score(test_Y, pred_bb)\n",
    "f_measure = f1_score(test_Y, pred_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headway\n",
    "accuracy = accuracy_score(bb_label, bb_pred)\n",
    "precision = precision_score(bb_label, bb_pred)\n",
    "recall = recall_score(bb_label, bb_pred)\n",
    "f_measure = f1_score(bb_label, bb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F-measure: \" + str(f_measure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot the residuals after fitting a linear model\n",
    "ax = sns.residplot(test_Y, pred, color=\"g\")\n",
    "ax.set_title('Residuals')\n",
    "ax.set_ylim([-600, 600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all error to see if there is standard or some big outliers\n",
    "plt.figure()\n",
    "plt.plot(test_Y, pred, 'ro', ms=0.5)\n",
    "# plt.ylim(10, 40)\n",
    "plt.xlabel('Headway (Label)')\n",
    "plt.ylabel('Predicted Headway')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = test_Y - np.array(pred).flatten()\n",
    "num_bins = 10\n",
    "width = 5\n",
    "height = 5\n",
    "plt.hist(diff, num_bins, facecolor='blue', alpha=0.5, log=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Total')\n",
    "plt.title('Distribution of the residual (label - prediction)')\n",
    "plt.rcParams[\"figure.figsize\"] = (width,height) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features importance\n",
    "\n",
    "#create dictionary\n",
    "f_imps = {}\n",
    "for i in range(len(features)):\n",
    "    f_imps[features[i]] = rf.feature_importances_[i]\n",
    "    \n",
    "#sort dictionary \n",
    "sorted_feature_names = sorted(f_imps, key=f_imps.__getitem__, reverse=True)\n",
    "sorted_values = sorted(f_imps.values(), reverse=True)\n",
    "\n",
    "num_to_print = 20\n",
    "for i in range(num_to_print):\n",
    "    print(\"%15s %4.3f\" % (sorted_feature_names[i], sorted_values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing error prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.loc[test_Y.index]\n",
    "\n",
    "wrong_preds = data_test[pred != test_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_bins = 10\n",
    "# width = 50\n",
    "# height = 25\n",
    "plt.hist(wrong_preds.route, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Rotas')\n",
    "plt.ylabel('Total')\n",
    "plt.title('Rotas que o modelo errou a predição')\n",
    "# plt.xticks(np.arange(0, 500, 1))\n",
    "# plt.rcParams[\"figure.figsize\"] = (width,height)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
